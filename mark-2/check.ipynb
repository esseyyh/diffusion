{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import csv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from transformers import CLIPTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldm.dpm.ddpm import DDPM\n",
    "from ldm.vae.encoder import Encoder\n",
    "from ldm.vae.decoder import Decoder\n",
    "from  ldm.clip.cliper import CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, csv_file):\n",
    "        self.root_dir = root_dir\n",
    "        self.csv_file = csv_file\n",
    "        gen=torch.Generator()\n",
    "        self.diff=DDPM(gen)\n",
    "        self.tokenizer = CLIPTokenizer(\"utils/vocab.json\", merges_file=\"utils/merges.txt\")\n",
    "\n",
    "        self.txt_encoder=CLIP()\n",
    "        self.txt.load\n",
    "        self.time_steps=self.diff.timesteps\n",
    "        self.encoder=Encoder()\n",
    "        self.transform= transforms.Compose([\n",
    "            transforms.Lambda(lambda t: (t/255)),\n",
    "            transforms.Lambda(lambda t: (t * 2) - 1),\n",
    "            transforms.Lambda(lambda t: t.permute([2,0,1])), # Scale data between [-1, 1] \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "        with open(self.csv_file, \"r\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            self.image_paths = []\n",
    "            self.txt=[]\n",
    "            \n",
    "            for row in reader:\n",
    "                image,txt = row[0], row[2]\n",
    "                self.image_paths.append(os.path.join(self.root_dir,image))\n",
    "                self.txt.append(txt)\n",
    "                \n",
    "                \n",
    "              \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.image_paths[index]\n",
    "        \n",
    "        t = torch.randint(0, self.time_steps,(1,))\n",
    "\n",
    "        image = torch.from_numpy(np.array((Image.open(image))))\n",
    "\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            image = self.transform(image)\n",
    "            noisy_image=self.diff.add_noise(image,t)\n",
    "            noisy_image=self.encoder(noisy_image)\n",
    "            text_guiance=self.txt_encoder(self.tokenizer.batch_encode_plus([self.txt[index]],padding=\"max_length\", max_length=77).input_ids)\n",
    "            \n",
    "\n",
    "        \n",
    "   \n",
    "\n",
    "        return image,self.encoder(noisy_image),t,text_guiance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CLIPTokenizer(\"utils/vocab.json\", merges_file=\"utils/merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    bloc1=Encoder()\n",
    "    #bloc1=torch.load('data/encoder_params.pt')\n",
    "    v=torch.randn(1,3,512,512)\n",
    "    v=bloc1(v)\n",
    "    v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"the quick brown fox jumps over the lazy hound\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_prompt=tokenizer.batch_encode_plus(\n",
    "                [prompt], padding=\"max_length\", max_length=77\n",
    "            ).input_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mephisto_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
